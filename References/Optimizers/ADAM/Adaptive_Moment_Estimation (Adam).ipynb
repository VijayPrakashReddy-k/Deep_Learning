{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `The Adam optimization algorithm is an extension to stochastic gradient descent(SGD) that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>How Does Adam Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Adam is different to classical stochastic gradient descent.\n",
    "\n",
    "`Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training.`\n",
    "\n",
    "        A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`The Adam method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Adam is Effective </h2>\n",
    "\n",
    "    Adam is a popular algorithm in the field of deep learning because it achieves good results fast.\n",
    "\n",
    "    Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`When introducing the algorithm, the authors list the attractive benefits of using Adam on non-convex optimization problems, as follows:`\n",
    "\n",
    "        Straightforward to implement.\n",
    "        \n",
    "        Computationally efficient.\n",
    "        \n",
    "        Little memory requirements.\n",
    "        \n",
    "        Invariant to diagonal rescale of the gradients.\n",
    "        \n",
    "        Well suited for problems that are large in terms of data and/or parameters.\n",
    "        \n",
    "        Appropriate for non-stationary objectives.\n",
    "        \n",
    "        Appropriate for problems with very noisy/or sparse gradients.\n",
    "        \n",
    "        Hyper-parameters have intuitive interpretation and typically require little tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Adam Configuration Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `alpha.`:\n",
    "\n",
    "            Also referred to as the learning rate or step size. The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial learning before the rate is updated. Smaller values (e.g. 1.0E-5) slow learning right down during training\n",
    "            \n",
    "- `beta1.`:\n",
    "        \n",
    "            The exponential decay rate for the first moment estimates (e.g. 0.9).\n",
    "\n",
    "- `beta2` :\n",
    "\n",
    "            The exponential decay rate for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient (e.g. NLP and computer vision problems).\n",
    "\n",
    "- `epsilon` :\n",
    "\n",
    "            Is a very small number to prevent any division by zero in the implementation (e.g. 10E-8).\n",
    "\n",
    "`Further, learning rate decay can also be used with Adam. The paper uses a decay rate alpha = alpha/sqrt(t) updted each epoch (t) for the logistic regression demonstration.`\n",
    "\n",
    "\n",
    "<h3>The Adam paper suggests:</h3>\n",
    "\n",
    "***`Good default settings for the tested machine learning problems are alpha=0.001, beta1=0.9, beta2=0.999 and epsilon=10âˆ’8`***\n",
    "\n",
    "`The TensorFlow documentation suggests some tuning of epsilon:`\n",
    "\n",
    "    The default value of 1e-8 for epsilon might not be a good default in general. For example, when training an Inception network on ImageNet a current good choice is 1.0 or 0.1.\n",
    "\n",
    "*We can see that the popular deep learning libraries generally use the default parameters recommended by the paper.*\n",
    "\n",
    "    TensorFlow: learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08.\n",
    "\n",
    "    Keras: lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0.\n",
    "\n",
    "    Blocks: learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-08, decay_factor=1.\n",
    "\n",
    "    Lasagne: learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08\n",
    "\n",
    "    Caffe: learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08\n",
    "\n",
    "    MxNet: learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8\n",
    "\n",
    "    Torch: learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Developers Views:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances.Its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In practice Adam is currently recommended as the default algorithm to use, and often works slightly better than RMSProp. However, it is often also worth trying SGD+Nesterov Momentum as an alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The two recommended updates to use are either SGD+Nesterov Momentum or Adam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
