{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`“Gradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function.”`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`“Stochastic”, in plain terms means “random”.`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align = 'center'>Stochastic Gradient Descent (optim.SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`We know that gradient descent is the rate of loss function w.r.t the weights a.k.a model parameters. The loss function can be a function of the mean square of the losses accumulated over the entire training dataset. Hence the weights are updated once at the end of each epoch. This results in reaching the exact minimum but requires heavy computation time/epochs to reach that point.`\n",
    "\n",
    "- On the other hand in SGD the weights are updated after looping via each training sample.\n",
    "\n",
    "                for i in range(nb_epochs):\n",
    "\n",
    "                np.random.shuffle(data)\n",
    "\n",
    "                    for example in data:\n",
    "\n",
    "                        params_grad = evaluate_gradient(loss_function, example, params)                                                                               \n",
    "                        \n",
    "                        params = params - learning_rate * params_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***From official documentation of pytorch SGD function has the following definition***\n",
    "\n",
    "        torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Arguments:***\n",
    "    \n",
    "- params (iterable) — iterable of parameters to optimize or dicts defining parameter groups\n",
    "\n",
    "- lr (float) — learning rate\n",
    "\n",
    "- momentum (float, optional) — momentum factor (default: 0)\n",
    "\n",
    "- weight_decay (float, optional) — weight decay (L2 penalty) (default: 0)\n",
    "\n",
    "- dampening (float, optional) — dampening for momentum (default: 0)\n",
    "\n",
    "- nesterov (bool, optional) — enables Nesterov momentum (default: False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `It is also common to sample a small number of data points instead of just one point at each step and that is called “mini-batch” gradient descent. Mini-batch tries to strike a balance between the goodness of gradient descent and speed of SGD.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><li>lr (float) — learning rate</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><li>momentum (float, optional) — momentum factor (default: 0)</h2>\n",
    "    \n",
    "https://distill.pub/2017/momentum/\n",
    "\n",
    "    Momentum or SGD with momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging. It is one of the most popular optimization algorithms and many state-of-the-art models are trained using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Normally we would see that while training the model, loss decreases immediately in the starting but gradually you reach a point when it seems you aren’t making any progress at all!. So what is wrong?\n",
    "\n",
    "        The problem could be the optimizer’s old nemesis, pathological curvature. Pathological curvature is, simply put, regions of f which aren’t scaled properly. The landscapes are often described as valleys, trenches, canals and ravines. The iterates either jump between valleys, or approach the optimum in small, timid steps. Progress along certain directions grind to a halt. In these unfortunate regions, gradient descent fumbles.\n",
    "\n",
    "Here using momemtum comes to the rescue. Correct value of momentum is obtained by cross validation and would avoid getting stuck in a local minima.\n",
    "\n",
    "                v = mu * v - learning_rate * dx # integrate velocity\n",
    "\n",
    "                x += v # integrate position "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local_Minima](local_minima.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><li>weight_decay (float, optional) — weight decay (L2 penalty) (default: 0)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dejanbatanjac.github.io/2019/07/02/Impact-of-WD.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><li>dampening (float, optional) — dampening for momentum (default: 0)</h2>  \n",
    "\n",
    "                        make less strong or intense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><li>Nesterov (bool, optional) — enables Nesterov momentum (default: False)</h2>\n",
    "\n",
    "    This type of momemtum has a slightly different methodology. Here weights update depend both on the classical momemtun and the gradient step in future with the present momemtum. The diagram below clearly depicts I’m trying to say."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            \n",
    "            x_ahead = x + mu * v\n",
    "        \n",
    "            # evaluate dx_ahead (the gradient at x_ahead instead of at x)\n",
    "            v = mu * v - learning_rate * dx_ahead\n",
    "            x += v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Nesterov Momentum](nesterov_momentum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`In fact it is said that SGD+Nesterov can be as good as Adam’s technique.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
