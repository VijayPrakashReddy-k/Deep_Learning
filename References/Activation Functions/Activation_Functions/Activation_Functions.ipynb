{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Activation_Function](activation_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Need of an activation function?***\n",
    " \n",
    "     If we do not apply an Activation function then the output signal would simply be a simple linear function. A linear function is just a polynomial of one degree. Now, a linear equation is easy to solve but they are limited in their complexity and have less power to learn complex functional mappings from data. A Neural Network without Activation function would simply be a Linear regression Model, which has limited power and does not perform well most of the times. We want our Neural Network to not just learn and compute a linear function but something more complicated than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Need of non-linearities?***\n",
    "\n",
    "    We need a Neural Network Model to learn and represent almost anything and any arbitrary complex function which maps inputs to outputs. Neural-Networks are considered Universal Function Approximators. It means that they can compute and learn any function at all. Almost any process we can think of can be represented as a functional computation in Neural Networks.\n",
    "\n",
    "     Hence it all comes down to this, we need to apply an Activation function f(x) so as to make the network more powerful and add the ability to it to learn something complex and complicated form data and represent non-linear complex arbitrary functional mappings between inputs and outputs. Hence using a non-linear Activation we are able to generate non-linear mappings from inputs to outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Activation Functions :</h3>\n",
    "\n",
    "                         ReLU, \n",
    "                             \n",
    "                         ReLU6, \n",
    "                         \n",
    "                         ELU, \n",
    "                         \n",
    "                         SELU, \n",
    "                         \n",
    "                         PReLU, \n",
    "                         \n",
    "                         LeakyReLU,\n",
    "                         \n",
    "                         Threshold, \n",
    "                         \n",
    "                         HardTanh, \n",
    "                         \n",
    "                         Sigmoid, \n",
    "                         \n",
    "                         Tanh,\n",
    "                                      \n",
    "                         LogSigmoid,\n",
    "                         \n",
    "                         Softplus, \n",
    "                         \n",
    "                         SoftShrink,\n",
    "                                    \n",
    "                         Softsign,\n",
    "                         \n",
    "                         TanhShrink, \n",
    "                         \n",
    "                         Softmin, \n",
    "                         \n",
    "                         Softmax,\n",
    "                               \n",
    "                         Softmax2d \n",
    "                         \n",
    "                      or LogSoftmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><li>Sigmoid</h3>\n",
    "\n",
    "        We used Sigmoid function for decades and faced with something called gradient descent or gradient explosion problems. This is how Sigmoid works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sigmoid Function](sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***When sigmoids didn't work, we felt maybe another complicated function, called TanH might work:***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><li> Hyperbolic Tanget Function</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hyperbolic_Tanget_function](tanh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    But neither did. \n",
    "\n",
    "    It turns out that the process of making a decision is not so complicated after all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><li> Rectified Linear Unit (ReLU)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `It is fast, simple and efficient.`\n",
    "\n",
    "\n",
    "- `It has become very popular in the past couple of years. It was recently proved that it had 6 times improvement in convergence from Tanh function.`\n",
    "\n",
    "      It’s just R(x) = max(0,x) \n",
    "      \n",
    "              i.e if x < 0 , R(x) = 0 and if x >= 0 , R(x) = x.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Relu](ReLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This is a very simple function. It allows all the positive numbers to pass on, as it is, and converts all the negatives to zero. It's message to backpropagation or kernels is simple. If you want some data to be passed on to the next layers, please make sure the values are positive. Negative values would be filtered out. This also means that if some value should not be passed on to the next layers, just convert them to negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReLU working Process](ReLU_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "   \n",
    "<h3><li>ReLU is linear (identity) for all positive values, and zero for all negative values. This means that:</h3>\n",
    "\n",
    "<ol>\n",
    "<li>It’s cheap to compute as there is no complicated math. The model can therefore take less time to train or run.\n",
    "\n",
    "<li>It converges faster. Linearity means that the slope doesn’t plateau, or “saturate,” when x gets large. It doesn’t have the vanishing gradient problem suffered by other activation functions like sigmoid or tanh.\n",
    "\n",
    "<li>It’s sparsely activated. Since ReLU is zero for all negative inputs, it’s likely for any given unit to not activate at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Relu_Example](relu_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    One might wonder, why such partiality to all the negative numbers? What if we allow a negative number to leak a bit of their values. Hence came Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><li>Leaky Relu </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Leaky_ReLU](leaky_relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    And then why stop there. You can complicate this further. Now instead of a being a constant we can get DNN to figure out what it should be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Leaky & Parametric Relu](leaky_parametric.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***And then came a sequence of papers, each just adding a character before ELU and creating new activation functions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SELU](selu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ELU](elu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SReLU](SReLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Swish](Swish.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Most of the times the innovators of these new Activation functions are using ReLU in later papers, that encouraging. There is no clear proof of which one is better. The innovators claim their's is better, but then later peer group would release their studies which claim otherwise. \n",
    "\n",
    "    ReLU is simple, efficient and fast, and even if any one of the above is better, we are talking about a marginal benefit, with an increase in computation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
