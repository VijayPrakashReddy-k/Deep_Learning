{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://thatbrguy.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inception_block](Inception_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Inception v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Salient parts in the image can have an extremely large variation in size. For instance, an image with a dog can be either of the following, as shown below. The area occupied by the dog is different in each image.\n",
    "\n",
    "- Because of this huge variation in the location of the information, choosing the right kernel size for the convolution operation becomes tough. A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally.\n",
    "\n",
    "- Very deep networks are prone to overfitting. It is also hard to pass gradient updates through the entire network\n",
    "\n",
    "- Naively stacking large convolution operations is computationally expensive.\n",
    " \n",
    "\n",
    "***Inception V1 solves this through:***\n",
    "\n",
    "- ****with multiple sized filters operating on the same level****\n",
    "\n",
    "- ****going wider to capture different GRF contexts****\n",
    "\n",
    "- ****not using addition, but concatenation to make sure GRF links are maintained till the SoftMax layer****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    This is where it all started. Let us analyze what problem it was purported to solve, and how it solved it. \n",
    "\n",
    "***The Premise:***\n",
    "\n",
    "`Salient parts in the image can have extremely large variation in size. For instance, an image with a dog can be either of the following, as shown below. The area occupied by the dog is different in each image.`\n",
    "\n",
    "![dog](dog.png)\n",
    "\n",
    "*From left: A dog occupying most of the image, a dog occupying a part of it, and a dog occupying very little space* \n",
    "\n",
    "    Because of this huge variation in the location of the information, choosing the right kernel size for the convolution operation becomes tough. \n",
    "    \n",
    " ***`A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally.`***\n",
    "\n",
    "    Very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network.\n",
    "\n",
    "    Naively stacking large convolution operations is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The Solution:***\n",
    "\n",
    "    Why not have filters with multiple sizes operate on the same level? The network essentially would get a bit “wider” rather than “deeper”. The authors designed the inception module to reflect the same.\n",
    "    \n",
    "    The below image is the “naive” inception module. It performs convolution on an input, with 3 different sizes of filters (1x1, 3x3, 5x5). Additionally, max pooling is also performed. The outputs are concatenated and sent to the next inception module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![V1_arch](v1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    As stated before, deep neural networks are computationally expensive. To make it cheaper, the authors limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions. Though adding an extra operation may seem counterintuitive, 1x1 convolutions are far more cheaper than 5x5 convolutions, and the reduced number of input channels also help. \n",
    "   \n",
    "`Do note that however, the 1x1 convolution is introduced after the max pooling layer, rather than before.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![V1_dimension_reduced](v1_dimension_reduction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Using the dimension reduced inception module, a neural network architecture was built. This was popularly known as GoogLeNet (Inception v1).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Net reference : ( 27 Layers )\n",
    "\n",
    "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/googlenet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GoogLeNet has 9 such inception modules stacked linearly. It is 22 layers deep (27, including the pooling layers). It uses global average pooling at the end of the last inception module.\n",
    "Needless to say, it is a pretty deep classifier. As with any very deep network, it is subject to the` ***`vanishing gradient problem`***.\n",
    "\n",
    "    To prevent the middle part of the network from “dying out”, the authors introduced two auxiliary classifiers. They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss. Weight value used in the paper was 0.3 for each auxiliary loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total loss used by the inception net during training.\n",
    "\n",
    "total_loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Needless to say, auxiliary loss is purely used for training purposes, and is ignored during inference.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Inception V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Inception v2 and Inception v3 were presented in the same paper. The authors proposed a number of upgrades which increased the accuracy and reduced the computational complexity. Inception v2 explores the following:`\n",
    "\n",
    "***The Premise:***\n",
    "\n",
    "`Reduce representational bottleneck. The intuition was that, neural networks perform better when convolutions didn’t alter the dimensions of the input drastically. Reducing the dimensions too much may cause loss of information, known as a` ***`“representational bottleneck”`***\n",
    "\n",
    "    Using smart factorization methods, convolutions can be made more efficient in terms of computational complexity.\n",
    "    \n",
    "***The Solution:***\n",
    "\n",
    "- `\"Factorize 5x5\" convolution to two 3x3 convolution operations to improve computational speed. Although this may seem counterintuitive, `***`a 5x5 convolution is 2.78 times more expensive than a 3x3 convolution`***. `So stacking two 3x3 convolutions infact leads to a boost in performance.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This is illustrated in the below image.\n",
    "    \n",
    "![Inception_V1_2_3x3_(5x5)](V1_2_3x3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Moreover, they \"factorize convolutions of filter size nxn\" to a combination of 1xn and nx1 convolutions. For example, a 3x3 convolution is equivalent to first performing a 1x3 convolution, and then performing a 3x1 convolution on its output. They found this method to be 33% more cheaper than the single 3x3 convolution.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This is illustrated in the below image.\n",
    "    \n",
    "![V1_nxn_1xn_nx1](V1_nxn_1xn_nx1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`The filter banks in the module were expanded (made wider instead of deeper) to remove the representational bottleneck. If the module was made deeper instead, there would be excessive reduction in dimensions, and hence loss of information.`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     This is illustrated in the below image.\n",
    "     \n",
    "![V2](Inception_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Inception v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The Premise :***\n",
    "\n",
    "`The authors noted that the auxiliary classifiers didn’t contribute much until near the end of the training process, when accuracies were nearing saturation. They argued that they function as regularizes, especially if they have BatchNorm or Dropout operations.`\n",
    "\n",
    "    Possibilities to improve on the Inception v2 without drastically changing the modules were to be investigated.\n",
    "\n",
    "***The Solution :***\n",
    "\n",
    "`Inception Net v3 incorporated all of the above upgrades stated for Inception v2, and in addition used the following:`\n",
    "\n",
    "    RMSProp Optimizer.\n",
    "    \n",
    "    Factorized 7x7 convolutions.\n",
    "    \n",
    "    BatchNorm in the Auxillary Classifiers.\n",
    "    \n",
    "    Label Smoothing (A type of regularizing component added to the loss formula that prevents the network from becoming too confident about a class. Prevents over fitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Inception v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***`Insanity : Repeating the same mistakes over and over again and expecting different results`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Inception v4 and Inception-ResNet were introduced in the same paper. For clarity, let us discuss them in separate sections.`\n",
    "\n",
    "***The Premise :***\n",
    "\n",
    "- `Make the modules more uniform. The authors also noticed that some of the modules were more complicated than necessary. This can enable us to boost performance by adding more of these uniform modules.`\n",
    "\n",
    "***The Solution :***\n",
    "\n",
    "- `The “stem” of Inception v4 was modified. The stem here, refers to the initial set of operations performed before introducing the Inception blocks.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inception_V4](Inception_v4.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The top image is the stem of Inception-ResNet v1. The bottom image is the stem of Inception v4 and Inception-ResNet v2.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`They had three main inception modules, named A,B and C (Unlike Inception v2, these modules are infact named A,B and C). They look very similar to their Inception v2 (or v3) counterparts.`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inception_V4_blocks](inception_v4_A_B_C.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(From left) Inception modules A,B,C used in Inception v4. Note how similar they are to the Inception v2 (or v3) modules.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Inception v4 introduced specialized`***`“Reduction Blocks”`***` which are used to change the width and height of the grid. The earlier versions didn’t explicitly have reduction blocks, but the functionality was implemented.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ResNet : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet Model](resnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Inspired by the performance of the ResNet, a hybrid inception module was proposed. There are two sub-versions of Inception ResNet, namely v1 and v2. Before we checkout the salient features, let us look at the minor differences between these two sub-versions.\n",
    "    \n",
    "- `Inception-ResNet v1 has a computational cost that is similar to that of Inception v3.`\n",
    "\n",
    "\n",
    "- `Inception-ResNet v2 has a computational cost that is similar to that of Inception v4.`\n",
    "\n",
    "    They have different stems, as illustrated in the Inception v4 section.\n",
    "\n",
    "`Both sub-versions have the same structure for the modules A, B, C and the reduction blocks.`***`Only difference is the hyper-parameter settings`***. `In this section, we’ll only focus on the structure. Refer to the paper for the exact hyper-parameter settings (The images are of Inception-Resnet v1).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There are two kinds of residual connections:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![residual_block](residual_block.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `1. The identity shortcuts (x) can be directly used when the input and output are of the same dimensions.`\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "*Residual block function when input and output dimensions are same*"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `2. When the dimensions change,` \n",
    "\n",
    "`A) The shortcut still performs identity mapping, with extra zero entries padded with the increased dimension.` \n",
    "\n",
    "`B) The projection shortcut is used to match the dimension (done by 1*1 conv)`\n",
    "\n",
    "*using the following formula*\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "    Residual block function when the input and output dimensions are not same.\n",
    "\n",
    "***`The first case adds no extra parameters, the second one adds in the form of W_{s}`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ResNet Architectures :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNetArchitectures](resnet_architectures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Each ResNet block is either 2 layer deep (Used in small networks like ResNet 18, 34) or 3 layer deep( ResNet 50, 101, 152).`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet_2_3_layers_block](resnet_2_3_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `The Bottleneck class implements a 3 layer block and Basicblock implements a 2 layer block. It also has implementations of all ResNet Architectures with pretrained weights trained on ImageNet.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations:***\n",
    "\n",
    "    ResNet Network Converges faster compared to plain counter part of it.\n",
    "\n",
    "    Identity vs Projection shorcuts. Very small incremental gains using projection shortcuts (Equation-2) in all the layers. So all ResNet blocks use only Identity shortcuts with Projections shortcuts used only when the dimensions changes.\n",
    "\n",
    "    ResNet-34 achieved a top-5 validation error of 5.71% better than BN-inception and VGG. ResNet-152 achieves a top-5 validation error of 4.49%. An ensemble of 6 models with different depths achieves a top-5 validation error of 3.57%. Winning the 1st place in ILSVRC-2015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
