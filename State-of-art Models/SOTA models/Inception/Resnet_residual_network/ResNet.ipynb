{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['residual_block.png',\n",
       " 'shallow and deeper_networks.png',\n",
       " 'resnet.png',\n",
       " 'resnet_ensemple_networks.png',\n",
       " 'resnet_architectures.png',\n",
       " 'imagenet_resnet_residual.png',\n",
       " 'Identity_mapping in residual blocks.png',\n",
       " 'resnet_2_3_layers.png']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "glob.glob(\"*.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet](resnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The authors of the ResNet paper argue, that the stacking layers shouldn't degrade the network performance, because we could simply stack identity mappings (layers that don't do anything) upon the current network, and the resulting architecture would perform the same.***\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This indicates that the deeper model should not produce a training error higher than its shallower counterparts.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenet_resnet_residual](imagenet_resnet_residual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    If you think about it, ResNet can be considered as an ensemble of Smaller networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Resnet_Ensemble](resnet_ensemple_networks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>WHERE IS THE RESIDUE?  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    But why call it residual? Where is the residue? It’s time we let the mathematicians within us to come to the surface. Let us consider a neural network block, whose input is x and we would like to learn the true distribution H(x). Let us denote the difference (or the residual) between this as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet_math](resnet_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet_core_idea](resnet_core_idea.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ResNet Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Resnet_Back_propagation](ResNet_back_propagation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `If λ ~ 0 (because of multiple positive ReLUs or other activation functions) the left term will be exponentially large, and gradient exploding problem occurs. As we should remember, when the gradient exploded, the loss cannot be converged.`\n",
    "\n",
    "- `If λ~0 (because of multiple ReLUs or other activation functions), the left term will be exponentially small, and the gradient vanishing problem occurs. We cannot update the gradient with a large value, the loss stays at the plateau and ends up converged with a large loss.`\n",
    "\n",
    "        Thus, that’s why we need to keep clean for the shortcut connection path from input to output without any Conv layers, BN and ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ResNet V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet_V2](resnet_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here's why!\n",
    "   \n",
    "![ResNet_math](resnet_m1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    the input signal xl is still kept alive! And"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet_math2](resnet_m2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    the gradients can (almost) never be zero!\n",
    "\n",
    "    The identity matrix summation speeds up the training process and improves gradient flow since the skip connections are taken from previous conv operations. Thus the backpropagation can effectively transfer error corrections to earlier layers much easier. This addresses the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`For ResNet, there are two kinds of residual connections:`\n",
    "\n",
    "- `1.The identity shortcuts (x) can be directly added with the input and output are of the same dimensions`\n",
    "\n",
    "- `2.when the dimensions change (input is larger than residual output, but we need to add them). The default way of solving this is to use a 1x1 Conv with a stride of 2. Yes, half of the pixels will be ignored.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    They do this because it's what you should be doing. Residual connections with different shapes should be handled via a learned linear transformation between the two tensors, e.g. a 1x1 convolution with appropriate strides and border_mode, or for Dense layers, just matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`We'll go through ResNet34.`\n",
    "\n",
    "    It has 1 convolution layer of 7x7 sized kernel (64), with a stride of 2\n",
    "    \n",
    "    It is followed by MaxPooling. In fact, ResNet has only 1 MaxPooling operation!\n",
    "    \n",
    "    It is followed by 4 ResNet blocks (config: 3, 4, 6, 3)\n",
    "    \n",
    "    The channels are constant in each block (64, 128, 256, 512 respectively). Each block has only 3x3 kernels.\n",
    "\n",
    "    The channel size is constant in each block\n",
    "    \n",
    "    Except for the first block, each block starts with a 3x3 kernel of stride 2 (this handles MaxPooling)\n",
    "\n",
    "    The dotted lines are 1x1 convs with stride 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet 34](ResNet34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SideNote:***\n",
    "\n",
    "    The accuracy of convolutional networks evaluated on ImageNet is vastly underestimated. We find that when the mistakes of the model as assessed by human subjects and considered correct when four out of five humans agree with the model's prediction, the top-1 error of a ResNet-101trained on ImageNet... decreases from 22.69% to 9.47%. Similarly, the top-5 error decreases from 6.44% to 1.94%. (This is true for other models as well). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet_Bottleneck](ResNet_bottleneck.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Deeper non-bottleneck ResNets also gain accuracy from increasing depth, but are not as economical as the bottleneck ResNets. So the usage of bottleneck designs is mainly due to practical considerations.` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                R18    - Regular        - 2, 2, 2, 2\n",
    "                \n",
    "                R34    - Regular        - 3, 4, 6, 3\n",
    "                \n",
    "                R50    - Bottleneck     - 3, 4, 6, 3\n",
    "                \n",
    "                R101   - Bottleneck     - 3, 4, 23, 3\n",
    "                \n",
    "                R152   - Bottleneck     - 3, 8, 36, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ResNet V3 or ResNeXt  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aggregated Transformations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ResNet V3_ResNext](ResNetV3_ResNext.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
